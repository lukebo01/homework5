{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca-\\AppData\\Local\\Temp\\ipykernel_28412\\1271226506.py:4: DtypeWarning: Columns (2,5,6,7,9,14,16,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mediated_df = pd.read_csv('final_mediated_schema.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV per l'addestramento generato con successo: training_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i file\n",
    "mediated_df = pd.read_csv('final_mediated_schema.csv')\n",
    "ground_truth_df = pd.read_csv('data/ground_truth.csv')\n",
    "matched_df = pd.read_csv('matched_companies_detailed.csv')\n",
    "\n",
    "# Funzione per controllare se una coppia Ã¨ nella ground truth\n",
    "def is_match(row, ground_truth):\n",
    "    left_company = row['company_name_left'].strip().lower()\n",
    "    right_company = row['company_name_right'].strip().lower()\n",
    "    \n",
    "    for _, gt in ground_truth.iterrows():\n",
    "        a = gt['company_a'].strip().lower()\n",
    "        b = gt['company_b'].strip().lower()\n",
    "\n",
    "        if (left_company == a and right_company == b) or (left_company == b and right_company == a):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Genera la colonna label\n",
    "matched_df['label'] = matched_df.apply(lambda row: is_match(row, ground_truth_df), axis=1)\n",
    "\n",
    "# Salva il CSV finale\n",
    "output_columns = ['company_name_left', 'industry_left', 'headquarters_city_left',\n",
    "                  'company_name_right', 'industry_right', 'headquarters_city_right', 'label']\n",
    "\n",
    "final_df = matched_df[output_columns]\n",
    "final_df.to_csv('training_data.csv', index=False)\n",
    "\n",
    "print(\"CSV per l'addestramento generato con successo: training_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne riordinate, righe vuote rimosse e file salvato come train.tsv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Carica il dataset\n",
    "file_path = 'entity-matching-transformer/data/training_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Porta la colonna 'label' al primo posto e rimuove eventuali valori mancanti\n",
    "cols = ['label'] + [col for col in df.columns if col != 'label']\n",
    "df = df[cols]\n",
    "df = df.dropna(subset=['label'])  # Rimuove righe senza etichetta\n",
    "\n",
    "# Salva il file come TSV senza righe vuote\n",
    "output_path = 'entity-matching-transformer/data/data/train.tsv'\n",
    "df.to_csv(output_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "print(\"Colonne riordinate, righe vuote rimosse e file salvato come train.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di coppie in intersezione: 24\n",
      "✅ Creato il file 'intersection_pairs.csv' con le coppie in intersezione e i valori di name_sim!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Caricamento dei file\n",
    "pairwise_file = 'matched_companies_detailed.csv'\n",
    "groundtruth_file = 'data/ground_truth.csv'\n",
    "\n",
    "pairwise = pd.read_csv(pairwise_file)\n",
    "groundtruth = pd.read_csv(groundtruth_file)\n",
    "\n",
    "# 2. Funzioni di normalizzazione\n",
    "def normalize_name(name):\n",
    "    return name.strip().lower() if isinstance(name, str) else str(name).lower()\n",
    "\n",
    "def normalize_pair(left_name, right_name):\n",
    "    left = normalize_name(left_name)\n",
    "    right = normalize_name(right_name)\n",
    "    return tuple(sorted([left, right]))  # es. ('a', 'b') invece di ('b', 'a')\n",
    "\n",
    "# 3. Aggiungiamo una colonna 'normalized_pair' a entrambi i DataFrame\n",
    "pairwise['normalized_pair'] = pairwise.apply(\n",
    "    lambda row: normalize_pair(row['company_name_left'], row['company_name_right']), axis=1\n",
    ")\n",
    "\n",
    "groundtruth['normalized_pair'] = groundtruth.apply(\n",
    "    lambda row: normalize_pair(row['company_a'], row['company_b']), axis=1\n",
    ")\n",
    "\n",
    "# 4. Trasformiamo in set per calcolare l'intersezione\n",
    "pairwise_set = set(pairwise['normalized_pair'])\n",
    "groundtruth_set = set(groundtruth['normalized_pair'])\n",
    "\n",
    "common_pairs = pairwise_set.intersection(groundtruth_set)\n",
    "print(f\"Numero di coppie in intersezione: {len(common_pairs)}\")\n",
    "\n",
    "# 5. Selezioniamo dal pairwise soltanto le righe che corrispondono all'intersezione\n",
    "intersection_df = pairwise[pairwise['normalized_pair'].isin(common_pairs)]\n",
    "\n",
    "# 6. Esportiamo il risultato in un nuovo CSV\n",
    "intersection_df.to_csv('intersection_pairs.csv', index=False)\n",
    "\n",
    "print(\"✅ Creato il file 'intersection_pairs.csv' con le coppie in intersezione e i valori di name_sim!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage\n",
    "import time\n",
    "\n",
    "# Avvio del timer\n",
    "start_time = time.time()\n",
    "\n",
    "# 1) Caricamento e pre-elaborazione dati\n",
    "schema_file = \"main_outputs/final_mediated_schema.csv\"\n",
    "df = pd.read_csv(schema_file)\n",
    "\n",
    "# Seleziona le colonne di interesse\n",
    "df = df[['company_name', 'industry', 'headquarters_city']]\n",
    "\n",
    "# Gestione dei valori mancanti: sostituzione con stringa vuota\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Normalizza gli attributi: rimozione degli spazi e conversione in minuscolo\n",
    "df = df.apply(lambda x: x.str.strip().str.lower() if x.dtype == 'object' else x)\n",
    "\n",
    "# Filtra i record indesiderati: rimuove righe in cui 'company_name' è vuoto o contiene solo '\\n'\n",
    "df = df[~(df['company_name'].str.strip().isin(['', '\\n']))]\n",
    "\n",
    "print(f\"Numero di record iniziali: {len(df)}\")\n",
    "\n",
    "# 2) Creazione dell'indice per il blocking: uso di SortedNeighbourhood sul campo 'company_name'\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.sortedneighbourhood(left_on=['company_name'], window=5)\n",
    "\n",
    "# Genera le coppie candidate\n",
    "candidate_pairs = indexer.index(df)\n",
    "print(f\"Numero di coppie candidate: {len(candidate_pairs)}\")\n",
    "\n",
    "# 3) Definizione delle regole di confronto: per ogni attributo viene applicato il confronto con Jaro-Winkler\n",
    "compare = recordlinkage.Compare()\n",
    "\n",
    "for col in df.columns:\n",
    "    compare.string(col, col, method='jarowinkler', label=f'{col}_similarity')\n",
    "\n",
    "# 4) Calcolo della matrice di similarità per le coppie candidate\n",
    "features = compare.compute(candidate_pairs, df)\n",
    "print(f\"Dimensioni della matrice di similarità: {features.shape}\")\n",
    "\n",
    "# 5) Imposta la similarità a zero se, per un determinato attributo, entrambe le celle sono vuote\n",
    "for col in df.columns:\n",
    "    empty_cells = (df[col] == '')\n",
    "    pairs_with_empty = candidate_pairs[\n",
    "        empty_cells.loc[candidate_pairs.get_level_values(0)].values &\n",
    "        empty_cells.loc[candidate_pairs.get_level_values(1)].values\n",
    "    ]\n",
    "    features.loc[pairs_with_empty, f'{col}_similarity'] = 0\n",
    "\n",
    "# 6) Filtraggio delle coppie candidate in base a regole di similarità:\n",
    "#    - 'company_name_similarity' deve essere > 0.7\n",
    "#    - Almeno 2 attributi devono avere una similarità >= soglia (0.75)\n",
    "threshold = 0.75\n",
    "matches = features[\n",
    "    (features['company_name_similarity'] < 0.9) & (features['company_name_similarity'] > 0.7) \n",
    "    #((features >= threshold).sum(axis=1) >= 2)\n",
    "]\n",
    "\n",
    "print(f\"Numero di coppie finali considerate 'match': {len(matches)}\")\n",
    "\n",
    "# 7) Unione con il dataframe originale per ottenere i record affiancati\n",
    "\n",
    "# a) Resetta l'indice (la MultiIndex diventa 'id_left' e 'id_right')\n",
    "matches = matches.reset_index()\n",
    "matches.rename(columns={'level_0': 'id_left', 'level_1': 'id_right'}, inplace=True)\n",
    "\n",
    "# b) Unisci i dati del record \"di sinistra\" (id_left)\n",
    "matches = matches.merge(df, left_on='id_left', right_index=True, how='left', suffixes=('', '_left'))\n",
    "\n",
    "# c) Unisci i dati del record \"di destra\" (id_right)\n",
    "matches = matches.merge(df, left_on='id_right', right_index=True, how='left', suffixes=('_left', '_right'))\n",
    "\n",
    "# (Opzionale) Elimina le coppie in cui il company_name è identico\n",
    "matches = matches[matches['company_name_left'] != matches['company_name_right']]\n",
    "\n",
    "# Salvataggio su file CSV\n",
    "matches.to_csv(\"matched_companies_detailed.csv\", index=False)\n",
    "print(\"✅ File 'matched_companies_detailed.csv' generato con i record affiancati!\")\n",
    "\n",
    "# Calcola e stampa il tempo di esecuzione\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tempo di esecuzione: {execution_time:.2f} secondi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca-\\AppData\\Local\\Temp\\ipykernel_28412\\1859461966.py:15: DtypeWarning: Columns (2,5,6,7,9,14,16,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_schema = pd.read_csv(schema_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di record nello schema mediato: 75793\n",
      "Numero di coppie nella Ground Truth: 205\n",
      "Numero di coppie candidate generate: 201353\n",
      "Distribuzione delle etichette candidate:\n",
      "label\n",
      "0    200959\n",
      "1       394\n",
      "Name: count, dtype: int64\n",
      "Positivi: 394, Negativi totali: 200959\n",
      "\n",
      "Report di classificazione (soglia ottimizzata):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.59      0.74       493\n",
      "           1       0.31      0.94      0.47        98\n",
      "\n",
      "    accuracy                           0.65       591\n",
      "   macro avg       0.65      0.76      0.60       591\n",
      "weighted avg       0.87      0.65      0.69       591\n",
      "\n",
      "Soglia ottimizzata: 0.20\n",
      "\n",
      "Numero di coppie filtrate dal modello con soglia 0.20: 78633\n",
      "\n",
      "✅ File 'matched_companies_filtered_gt.csv' generato con solo le coppie che il modello considera come match (simili alla GT)!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################################\n",
    "# 1. Caricamento e pre-elaborazione dati     #\n",
    "##############################################\n",
    "\n",
    "# Caricamento dello schema mediato\n",
    "schema_file = \"main_outputs/final_mediated_schema.csv\"\n",
    "df_schema = pd.read_csv(schema_file)\n",
    "df_schema = df_schema[['company_name']].copy()\n",
    "df_schema.dropna(subset=['company_name'], inplace=True)\n",
    "df_schema['company_name'] = df_schema['company_name'].str.strip().str.lower()\n",
    "\n",
    "# Creiamo una nuova colonna con il primo carattere per il blocking\n",
    "df_schema['first_letter'] = df_schema['company_name'].str[0]\n",
    "\n",
    "print(f\"Numero di record nello schema mediato: {len(df_schema)}\")\n",
    "\n",
    "# Caricamento della Ground Truth\n",
    "gt_file = \"data/ground_truth.csv\"\n",
    "df_gt = pd.read_csv(gt_file)\n",
    "for col in ['company_a', 'company_b']:\n",
    "    df_gt[col] = df_gt[col].fillna(\"\").str.strip().str.lower()\n",
    "\n",
    "# Creiamo una chiave ordinata per ciascuna coppia della GT\n",
    "df_gt['pair'] = df_gt.apply(lambda row: tuple(sorted([row['company_a'], row['company_b']])), axis=1)\n",
    "gt_pairs = set(df_gt['pair'])\n",
    "print(f\"Numero di coppie nella Ground Truth: {len(gt_pairs)}\")\n",
    "\n",
    "##############################################\n",
    "# 2. Generazione delle coppie candidate (blocking)  #\n",
    "##############################################\n",
    "\n",
    "# Usiamo un blocking sul primo carattere del company_name per ridurre il numero di coppie candidate\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block(left_on='first_letter')\n",
    "candidate_pairs = indexer.index(df_schema)\n",
    "print(f\"Numero di coppie candidate generate (blocking): {len(candidate_pairs)}\")\n",
    "\n",
    "##############################################\n",
    "# 3. Calcolo delle feature di similarità       #\n",
    "##############################################\n",
    "\n",
    "# Calcoliamo la similarità jarowinkler con recordlinkage\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string('company_name', 'company_name', method='jarowinkler', label='jarowinkler')\n",
    "features_df = compare.compute(candidate_pairs, df_schema)\n",
    "features_df = features_df.reset_index()  # colonne: level_0, level_1, jarowinkler\n",
    "\n",
    "# Aggiungiamo la feature \"seqsim\" calcolata con SequenceMatcher\n",
    "def seq_similarity(idx_left, idx_right):\n",
    "    name_left = df_schema.loc[idx_left, 'company_name']\n",
    "    name_right = df_schema.loc[idx_right, 'company_name']\n",
    "    return SequenceMatcher(None, name_left, name_right).ratio()\n",
    "\n",
    "features_df['seqsim'] = features_df.apply(lambda row: seq_similarity(row['level_0'], row['level_1']), axis=1)\n",
    "\n",
    "##############################################\n",
    "# 4. Costruzione del dataset di training      #\n",
    "##############################################\n",
    "\n",
    "# Creiamo una chiave per ciascuna coppia candidata\n",
    "features_df['pair'] = features_df.apply(lambda row: tuple(sorted([\n",
    "    df_schema.loc[row['level_0'], 'company_name'],\n",
    "    df_schema.loc[row['level_1'], 'company_name']\n",
    "])), axis=1)\n",
    "\n",
    "# Etichettiamo: 1 se la coppia è presente nella GT, 0 altrimenti\n",
    "features_df['label'] = features_df['pair'].apply(lambda p: 1 if p in gt_pairs else 0)\n",
    "\n",
    "print(\"Distribuzione delle etichette candidate:\")\n",
    "print(features_df['label'].value_counts())\n",
    "\n",
    "# Campioniamo i negativi per evitare uno sbilanciamento eccessivo\n",
    "positives = features_df[features_df['label'] == 1]\n",
    "negatives = features_df[features_df['label'] == 0]\n",
    "print(f\"Positivi: {len(positives)}, Negativi totali: {len(negatives)}\")\n",
    "\n",
    "n_sample_neg = min(len(negatives), len(positives) * 5)\n",
    "negatives_sample = negatives.sample(n=n_sample_neg, random_state=42)\n",
    "\n",
    "df_train = pd.concat([positives, negatives_sample], axis=0).reset_index(drop=True)\n",
    "X = df_train[['jarowinkler', 'seqsim']]\n",
    "y = df_train['label']\n",
    "\n",
    "##############################################\n",
    "# 5. Addestramento del classificatore          #\n",
    "##############################################\n",
    "\n",
    "# Dividiamo in training e validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Calcoliamo le probabilità sul validation set\n",
    "y_val_probs = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Ottimizziamo la soglia per massimizzare l'F1 score sul validation set\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= thresh).astype(int)\n",
    "    f1_scores.append(f1_score(y_val, y_pred_thresh))\n",
    "\n",
    "best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "print(\"\\nReport di classificazione (con soglia ottimizzata sul validation set):\")\n",
    "y_pred_opt = (y_val_probs >= best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_opt))\n",
    "print(f\"Soglia ottimizzata: {best_thresh:.2f}\")\n",
    "\n",
    "# Visualizziamo la distribuzione delle probabilità di match\n",
    "plt.hist(clf.predict_proba(X)[:, 1], bins=50)\n",
    "plt.xlabel(\"Probabilità di match\")\n",
    "plt.ylabel(\"Frequenza\")\n",
    "plt.title(\"Distribuzione delle probabilità di match nel training set\")\n",
    "plt.show()\n",
    "\n",
    "##############################################\n",
    "# 6. Applicazione del modello a tutte le coppie  #\n",
    "##############################################\n",
    "\n",
    "X_all = features_df[['jarowinkler', 'seqsim']]\n",
    "features_df['match_prob'] = clf.predict_proba(X_all)[:, 1]\n",
    "\n",
    "# Invece di usare direttamente la soglia ottimizzata, possiamo impostare una soglia più stringente per ridurre i match.\n",
    "manual_thresh = 0.75\n",
    "df_filtered = features_df[features_df['match_prob'] >= manual_thresh].copy()\n",
    "print(f\"\\nNumero di coppie filtrate con soglia manuale {manual_thresh}: {len(df_filtered)}\")\n",
    "\n",
    "##############################################\n",
    "# 7. Salvataggio del risultato               #\n",
    "##############################################\n",
    "\n",
    "# Uniamo i dettagli dello schema mediato alle coppie filtrate\n",
    "df_filtered = df_filtered.merge(df_schema[['company_name']], left_on='level_0', right_index=True, how='left', suffixes=('', '_left'))\n",
    "df_filtered = df_filtered.merge(df_schema[['company_name']], left_on='level_1', right_index=True, how='left', suffixes=('_left', '_right'))\n",
    "\n",
    "output_file = \"matched_companies_filtered_gt.csv\"\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "print(f\"\\n✅ File '{output_file}' generato con le coppie che il modello considera come match, utilizzando blocking e soglia {manual_thresh}!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
